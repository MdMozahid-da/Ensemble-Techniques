{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Techniques"
      ],
      "metadata": {
        "id": "q3nfgi8bIW5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        " - Yes, Bagging can be effectively used for regression tasks. The Bagging Regressor, a common example, applies bootstrap aggregation to regression models like Decision Trees. This helps reduce variance and improves stability, particularly in datasets with noisy features. The key idea is that multiple models are trained independently on different subsets of the dataset, and their predictions are averaged to produce a final output. By averaging across multiple models, Bagging ensures better generalization compared to a single regressor. It is commonly used in financial predictions, climate modeling, and medical data analysis."
      ],
      "metadata": {
        "id": "pPyBPFJUIZS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training?\n",
        " - Single model training involves building and fine-tuning one algorithm on the data, relying solely on its ability to generalize. In contrast, multiple model training—often called ensemble learning—combines multiple models to improve performance.\n",
        "\n",
        "      - Single model training may suffer from overfitting (memorizing noise) or underfitting (missing patterns).\n",
        "\n",
        "      - Multiple model training reduces bias and variance by leveraging diverse models that complement each other. Common ensemble methods include Bagging (Random Forest), Boosting (XGBoost, AdaBoost), and Stacking (layering models for refined predictions). These approaches are widely used in machine learning competitions and real-world applications like fraud detection and medical diagnostics."
      ],
      "metadata": {
        "id": "l2HzkWjrJOlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest.\n",
        " - Feature randomness in Random Forest is a technique used to introduce diversity among decision trees, improving robustness and accuracy. Instead of allowing each tree to consider all features, Random Forest selects a random subset of features at each split. This prevents trees from relying too heavily on dominant features and ensures a more balanced decision-making process.\n",
        "\n",
        "    - Advantages: Helps reduce overfitting, as trees don’t all learn from the same dominant features.\n",
        "\n",
        "    - Applications: Used in predictive modeling tasks like stock price forecasting, image classification, and customer segmentation. Overall, feature randomness ensures that models learn from different perspectives, enhancing their predictive power."
      ],
      "metadata": {
        "id": "aPQdmZjLJbXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score?\n",
        " - OOB Score is a validation method for Random Forest, allowing accuracy measurement without requiring cross-validation. Since each tree in the Random Forest is trained on a bootstrap sample, some data points are left out (out-of-bag samples). These excluded samples are used to evaluate the model’s performance without additional data splitting.\n",
        "\n",
        "     - Why is it useful? It provides an unbiased error estimate for Random Forest models.\n",
        "\n",
        "     - How is it calculated? Each out-of-bag sample is tested against the trees that did NOT use it for training, improving generalization estimation.\n",
        "\n",
        "     - Key Benefit: Saves computation time while still providing reliable accuracy estimates. OOB evaluation is widely used in financial modeling, medical diagnosis, and risk assessment applications."
      ],
      "metadata": {
        "id": "C3l8Np7JJigW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model?\n",
        " - Random Forest measures feature importance by assessing how much a feature contributes to the model’s predictions. There are two main methods:\n",
        "\n",
        "      - Gini Importance (Mean Decrease in Impurity): This method evaluates how much a feature helps in correctly splitting data across all trees. Features with higher Gini Importance contribute more to correct decisions.\n",
        "\n",
        "     - Permutation Importance: This method randomly shuffles feature values and measures the impact on model accuracy. If accuracy drops significantly, the feature is considered important. Feature importance is useful for selecting the most relevant features, simplifying models, and improving performance in tasks like medical diagnosis, finance predictions, and fraud detection."
      ],
      "metadata": {
        "id": "WfVOiE_zLM6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier.\n",
        " - A Bagging Classifier works by training multiple base models (often Decision Trees) independently on random bootstrap samples from the dataset. These models then make predictions, which are combined through majority voting for classification or averaging for regression.\n",
        "\n",
        "       - Each model in the ensemble is trained on a different subset of data, which helps reduce overfitting.\n",
        "\n",
        "       - Since the models operate independently, Bagging improves stability and ensures better generalization to unseen data.\n",
        "\n",
        "       - Bagging is commonly used in Random Forest, where multiple decision trees collectively make predictions for robust outcomes in applications like customer segmentation and risk analysis."
      ],
      "metadata": {
        "id": "1LFyC61sLUPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        " - The performance of a Bagging Classifier is measured using standard classification metrics:\n",
        "\n",
        "     - Accuracy: Overall percentage of correct predictions.\n",
        "\n",
        "     - Precision: Measures how many predicted positive labels are actually correct.\n",
        "\n",
        "     - Recall: Measures the classifier’s ability to identify all actual positives.\n",
        "\n",
        "     - F1-score: A balanced metric combining precision and recall. Additionally, the Out-of-Bag (OOB) Score can be used to assess performance without needing cross-validation, making evaluations more efficient. The effectiveness of Bagging is proven in fields like healthcare diagnostics, fraud detection, and recommendation systems."
      ],
      "metadata": {
        "id": "51-fzfklUQ28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work?\n",
        " - A Bagging Regressor follows the same principles as a Bagging Classifier but is designed for continuous numerical predictions instead of classification. Here’s how it works:\n",
        "\n",
        "    - It trains multiple regressors (typically Decision Trees) on random subsets of data using bootstrap sampling.\n",
        "\n",
        "     - Each regressor makes independent predictions.\n",
        "\n",
        "     - The final prediction is made by averaging the outputs of all models, leading to a more stable and accurate result.\n",
        "\n",
        "    Why is it useful?\n",
        "    - Reduces variance: Unlike a single decision tree, Bagging prevents the model from fitting too closely to noisy data.\n",
        "\n",
        "    - Handles complex relationships: Works well in cases where single models struggle with high variance, such as financial forecasting, climate prediction, and medical diagnostics.\n",
        "\n",
        "    - Improves robustness: By averaging multiple predictions, Bagging ensures the output is less sensitive to extreme values and anomalies.\n",
        "\n",
        "    - Bagging Regressors are widely used in real estate price predictions, stock market analysis, and demand forecasting, where stability is crucial."
      ],
      "metadata": {
        "id": "qN4WSbviUZCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques?\n",
        " - Ensemble techniques increase accuracy, stability, and robustness by leveraging multiple models. Their key benefits include:\n",
        "\n",
        "    - Improved accuracy: Combining multiple models reduces errors, leading to better predictions.\n",
        "\n",
        "     - Reduced overfitting: Ensemble methods generalize better on unseen data by reducing reliance on any single model.\n",
        "\n",
        "     - Handles complex datasets: Works well in high-dimensional and imbalanced data scenarios.\n",
        "\n",
        "     - Better decision-making: Different models capture various aspects of the data, leading to more reliable predictions.\n",
        "\n",
        "    - Flexibility across domains: Applied in fraud detection, healthcare analytics, image recognition, and natural language processing.\n",
        "\n",
        "    - For example, Random Forest (Bagging-based) and XGBoost (Boosting-based) outperform single classifiers in credit scoring, ensuring fair risk assessments."
      ],
      "metadata": {
        "id": "bD2rbRoXUhAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods?\n",
        " - Despite their advantages, ensemble methods pose several challenges:\n",
        "\n",
        "      - Computational Complexity: Training multiple models requires more memory and processing power than a single algorithm.\n",
        "\n",
        "      - Difficulty in Interpretation: Unlike simpler models (e.g., linear regression), ensemble models lack transparency, making it harder to explain predictions.\n",
        "\n",
        "      - Risk of Overfitting: Boosting techniques, such as AdaBoost, continuously adjust based on errors, which can lead to overfitting if not tuned carefully.\n",
        "\n",
        "      - Time-Consuming Hyperparameter Tuning: Methods like Random Forest, XGBoost, and Stacking require careful tuning to maximize efficiency. However, techniques like feature importance analysis and simplified visualization methods can help mitigate these concerns."
      ],
      "metadata": {
        "id": "L6mHoIkFUlD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques.\n",
        " - The fundamental idea of ensemble techniques is leveraging multiple models to improve predictive accuracy by:\n",
        "\n",
        "     - Combining diverse models: Different models learn various data patterns, making the overall prediction stronger.\n",
        "\n",
        "     - Reducing errors: Averaging multiple predictions minimizes bias and variance, leading to stable predictions.\n",
        "\n",
        "     - Correcting mistakes: Boosting techniques, such as Gradient Boosting, learn from previous errors and adjust the model accordingly.\n",
        "\n",
        "     - Enhancing generalization: Helps models perform better on unseen test data, reducing sensitivity to outliers.\n",
        "\n",
        "     For example, self-driving car technology relies on ensemble methods combining object detection models, depth estimation algorithms, and reinforcement learning systems to make accurate driving decisions."
      ],
      "metadata": {
        "id": "TATPtL8JU5sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?\n",
        " - A Random Forest Classifier is an ensemble learning model that builds multiple decision trees and combines their predictions through majority voting (classification) or averaging (regression).\n",
        "\n",
        "   How it works:\n",
        "\n",
        "        Bootstrap Sampling: Each tree is trained on a random subset of data.\n",
        "\n",
        "        Feature Randomness: At every split, a random subset of features is selected, promoting diversity.\n",
        "\n",
        "        Aggregation: Predictions from all trees are combined to make a final decision.\n",
        "\n",
        "    Advantages:\n",
        "\n",
        "        Lower variance: Unlike single decision trees, Random Forest generalizes better.\n",
        "\n",
        "       Prevents overfitting: Multiple trees ensure more balanced predictions.\n",
        "\n",
        "        High accuracy: Works well on complex datasets.\n",
        "\n",
        "Applications:\n",
        "\n",
        "Used in fraud detection, customer segmentation, medical diagnosis, and recommendation systems, where reliable classifications are necessary."
      ],
      "metadata": {
        "id": "biXBhHJOU8gT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques?\n",
        " - Ensemble learning consists of three key techniques:\n",
        "\n",
        "     - Bagging (Bootstrap Aggregating)\n",
        "\n",
        "            Trains multiple models independently on different subsets of data.\n",
        "\n",
        "            Reduces variance, preventing overfitting.\n",
        "\n",
        "            Example: Random Forest, which averages multiple decision trees.\n",
        "\n",
        "     - Boosting\n",
        "\n",
        "            Trains models sequentially, correcting errors from previous models.\n",
        "\n",
        "            Reduces bias by improving weak learners step by step.\n",
        "\n",
        "            Example: AdaBoost, XGBoost, which enhance accuracy by focusing on misclassified samples.\n",
        "\n",
        "     - Stacking\n",
        "\n",
        "            Combines multiple models of different types and trains a final meta-model for optimized decision-making.\n",
        "\n",
        "            Example: Stacked Generalization, where base models feed their outputs into a neural network or logistic regression for improved accuracy.\n",
        "\n",
        "     Each method is widely used in fraud detection, recommendation systems, and medical imaging to enhance predictive reliability."
      ],
      "metadata": {
        "id": "mSFSXIbSVWqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning?\n",
        " - Ensemble learning is a powerful approach that combines multiple models to improve prediction accuracy, reduce errors, and enhance generalization.\n",
        "\n",
        "     Why is it useful?\n",
        "\n",
        "       - A single model may suffer from high bias (too simple) or high variance (overfits).\n",
        "\n",
        "       - Ensemble methods balance bias and variance by using multiple models collaboratively.\n",
        "\n",
        "       - The resulting predictions are more reliable and robust than any standalone algorithm.\n",
        "\n",
        "     Common applications:\n",
        "        - Spam filtering: Uses multiple classifiers to correctly flag spam emails.\n",
        "\n",
        "        - Stock market prediction: Employs Random Forest and Boosting to analyze trends.\n",
        "\n",
        "        - Medical diagnosis: Uses ensemble models to assess symptoms and improve disease classification.\n",
        "\n",
        "     By combining weak predictors into a strong model, ensemble learning has become essential in AI-driven decision-making across industries."
      ],
      "metadata": {
        "id": "wX19Nwp9VaiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods?\n",
        " - While ensemble learning is powerful, it is not always the best choice. Avoid using it when:\n",
        "\n",
        "     - Dataset is small: With limited data, simpler models can generalize well without needing multiple learners.\n",
        "\n",
        "     - Computational resources are limited: Training multiple models requires high memory and processing power, which can be impractical.\n",
        "\n",
        "     - Interpretability is needed: If decision-making transparency is crucial (e.g., in medicine or law), simpler models like logistic regression may be preferred.\n",
        "\n",
        "     - Problem complexity is low: If a problem can be solved using a single well-tuned model, ensemble methods might be unnecessary.\n",
        "\n",
        "     Despite these limitations, ensemble techniques remain indispensable in high-stakes fields like cybersecurity, financial risk assessment, and deep learning applications."
      ],
      "metadata": {
        "id": "HEoNiCHdVd0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting?\n",
        " - Bagging reduces overfitting by training multiple models on different subsets of data, ensuring they don’t memorize specific patterns. Here’s how:\n",
        "\n",
        "      - Bootstrap Sampling: Each model is trained on random subsets instead of the entire dataset, preventing over-dependence on any one feature.\n",
        "\n",
        "      - Variance Reduction: Since models work independently, their predictions are averaged, eliminating extreme fluctuations.\n",
        "\n",
        "     - Prevents Model Memorization: Decision Trees tend to overfit, but Bagging (e.g., Random Forest) smooths out noisy variations by blending multiple trees’ predictions.\n",
        "\n",
        "     Example:\n",
        "\n",
        "     Imagine predicting house prices:\n",
        "\n",
        "      - A single tree might base decisions on one dominant feature (e.g., square footage), ignoring other factors.\n",
        "\n",
        "      - Bagging ensures different trees focus on different attributes (e.g., location, number of bedrooms), leading to balanced and unbiased predictions."
      ],
      "metadata": {
        "id": "HSMVImzXVho_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree?\n",
        " - A single Decision Tree works by making hierarchical decisions based on features, but it has weaknesses like overfitting and high variance when trained on complex datasets. In contrast, Random Forest improves upon this by:\n",
        "\n",
        "     - Reducing Variance: Random Forest builds multiple trees trained on random subsets of data, averaging their predictions for stability.\n",
        "\n",
        "     - Minimizing Overfitting: Decision Trees often memorize training data; Random Forest ensures diversity in tree structures, preventing excessive reliance on specific patterns.\n",
        "\n",
        "     - Improving Accuracy: A single tree may make inconsistent decisions, while multiple trees in Random Forest produce more reliable predictions.\n",
        "\n",
        "     - Handling Missing Values: Random Forest processes missing data better, making it robust for real-world applications.\n",
        "\n",
        "     - Feature Importance Analysis: Decision Trees alone do not provide strong insights into influential features, whereas Random Forest ranks important features, aiding interpretability."
      ],
      "metadata": {
        "id": "8j-DW_H_VlBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging?\n",
        " - Bootstrap sampling is the foundation of Bagging (Bootstrap Aggregating), and it works as follows:\n",
        "\n",
        "     - Random Sampling with Replacement: Each base model is trained on a random subset of data, created by sampling with replacement (some samples may appear multiple times).\n",
        "\n",
        "     - Diversity in Learning: Since each model sees different versions of the dataset, Bagging ensures that individual models capture varied patterns, leading to a balanced final prediction.\n",
        "\n",
        "     - Reduces Model Overfitting: Training multiple models prevents them from memorizing noise and enhances generalization on unseen data.\n",
        "\n",
        "     Example:\n",
        "\n",
        "     Consider predicting loan approvals using customer data.\n",
        "\n",
        "     - A single decision tree might focus heavily on income level while ignoring other factors.\n",
        "\n",
        "     - By bootstrap sampling, Bagging ensures different models focus on attributes like credit score, employment history, and debt-to-income ratio, resulting in better decisions."
      ],
      "metadata": {
        "id": "sp7nDgYUVowr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques?\n",
        " - Ensemble methods are widely used due to their reliability and accuracy in complex decision-making tasks. Some key applications include:\n",
        "\n",
        "     - Spam Filtering (Email Security):\n",
        "\n",
        "         Algorithms like Naïve Bayes + Random Forest help classify emails as spam or legitimate by evaluating word frequency, metadata, and sender credibility.\n",
        "\n",
        "     - Fraud Detection (Banking & Cybersecurity):\n",
        "\n",
        "          Ensemble models combine anomaly detection techniques, tracking unusual transactions and securing banking operations from fraudsters.\n",
        "\n",
        "     - Medical Diagnosis (Disease Prediction):\n",
        "\n",
        "        Boosting algorithms (e.g., XGBoost, Gradient Boosting) analyze patient symptoms to detect diseases such as cancer or heart disorders, improving early diagnosis accuracy.\n",
        "\n",
        "     - Stock Market & Business Forecasting:\n",
        "\n",
        "       Ensemble models analyze multiple economic indicators, merging results from decision trees, regression models, and deep learning algorithms to predict stock prices accurately.\n",
        "\n",
        "     - Self-driving Cars (Computer Vision):\n",
        "\n",
        "       Autonomous vehicles use ensemble models combining object detection, depth estimation, and behavioral analysis to make real-time navigation decisions."
      ],
      "metadata": {
        "id": "ZpfINIrgVr64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?\n",
        " - Bagging and Boosting are two powerful ensemble learning techniques, but they work in fundamentally different ways.\n",
        "\n",
        "     - Bagging (Bootstrap Aggregating) reduces variance by training multiple models independently on different random subsets of the dataset. These models make predictions separately, and their results are averaged (for regression) or combined through majority voting (for classification). Bagging is useful for improving stability, especially when using high-variance models like Decision Trees. Random Forest is a well-known example of Bagging.\n",
        "\n",
        "     - Boosting, on the other hand, reduces bias by training models sequentially. Each model learns from the errors of the previous one, adjusting weights to give more importance to misclassified examples. This step-by-step refinement process makes Boosting excellent for increasing predictive accuracy. However, if not carefully tuned, Boosting can sometimes lead to overfitting. AdaBoost and XGBoost are popular examples of Boosting."
      ],
      "metadata": {
        "id": "EhEE4kE1VvOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "Q2-f7p9EkUyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "2Je_0EVBQkO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a BaggingRegressor using DecisionTreeRegressors as base estimators\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
        "                           n_estimators=10,\n",
        "                           random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute Mean Absolute Error\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhXZvvoGDLnE",
        "outputId": "9d062fa7-e86c-49c8-c1cf-eb59888859da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 21.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "wM6h2hfiDMBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset and split into train/test sets.\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Single model training using Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Multiple model training using Random Forest (ensemble of trees)\n",
        "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {acc_dt:.2f}\")\n",
        "print(f\"Random Forest Accuracy: {acc_rf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtBcc0VWDW57",
        "outputId": "8e719ef3-5dbb-4b97-c00b-83b6c57ccb9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "Random Forest Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "vsWQSN6CDXXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "import pandas as pd\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train a Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Retrieve and display feature importances.\n",
        "feature_importances = rf.feature_importances_\n",
        "features = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSr9HkWaDcBM",
        "outputId": "ac3aea7f-617d-4764-9555-682b6286c787"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Feature  Importance\n",
            "6    Feature 6    0.350438\n",
            "0    Feature 0    0.173790\n",
            "12  Feature 12    0.110384\n",
            "9    Feature 9    0.108094\n",
            "11  Feature 11    0.107969\n",
            "4    Feature 4    0.043374\n",
            "8    Feature 8    0.030846\n",
            "1    Feature 1    0.028194\n",
            "3    Feature 3    0.013923\n",
            "5    Feature 5    0.012801\n",
            "10  Feature 10    0.010271\n",
            "2    Feature 2    0.007858\n",
            "7    Feature 7    0.002058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "Ho3pucMmDcsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train Random Forest with oob_score enabled\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "print(f\"OOB Score: {rf.oob_score_:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-JzjHjXDm3e",
        "outputId": "eceacfe3-99f1-45e2-f3e4-77a85e19ea92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "berzIOutDnST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Measure feature importance\n",
        "importances = rf.feature_importances_\n",
        "features = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_5CUbIxDrMN",
        "outputId": "372b7a7c-c69d-4148-dd7c-937701781969"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Feature  Importance\n",
            "23  Feature 23    0.139357\n",
            "27  Feature 27    0.132225\n",
            "7    Feature 7    0.107046\n",
            "20  Feature 20    0.082848\n",
            "22  Feature 22    0.080850\n",
            "2    Feature 2    0.067990\n",
            "6    Feature 6    0.066917\n",
            "3    Feature 3    0.060462\n",
            "26  Feature 26    0.037339\n",
            "0    Feature 0    0.034843\n",
            "13  Feature 13    0.029553\n",
            "25  Feature 25    0.019864\n",
            "21  Feature 21    0.017485\n",
            "1    Feature 1    0.015225\n",
            "10  Feature 10    0.014264\n",
            "24  Feature 24    0.012232\n",
            "5    Feature 5    0.011597\n",
            "12  Feature 12    0.010085\n",
            "28  Feature 28    0.008179\n",
            "4    Feature 4    0.007958\n",
            "19  Feature 19    0.005942\n",
            "16  Feature 16    0.005820\n",
            "15  Feature 15    0.005612\n",
            "14  Feature 14    0.004722\n",
            "29  Feature 29    0.004497\n",
            "17  Feature 17    0.003760\n",
            "11  Feature 11    0.003744\n",
            "18  Feature 18    0.003546\n",
            "8    Feature 8    0.003423\n",
            "9    Feature 9    0.002615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "l9-TVwWXDrm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Trees as base estimator\n",
        "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=10,\n",
        "                            random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x99MX-hDu-X",
        "outputId": "cbafc874-2998-4817-e688-ee192a454168"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "pNJ00WHYDvpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset and split\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=10,\n",
        "                            random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vyohHraDzKC",
        "outputId": "e020f500-afb0-42fe-b220-4b53dec4d8cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "RwHzJzd4Dzjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model performance\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjeSOLowD2U3",
        "outputId": "e956621a-2309-4acc-d96a-ba86c2897a63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 871.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "uec30wsiD2s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset and split\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Single model: Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "acc_dt = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Ensemble: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "acc_rf = accuracy_score(y_test, rf.predict(X_test))\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {acc_dt:.2f}\")\n",
        "print(f\"Random Forest Accuracy: {acc_rf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKWmWPlqD5T2",
        "outputId": "1df24247-da67-4344-9587-7eef6291efb6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.94\n",
            "Random Forest Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "WqTCdiwFD5se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Time a single Decision Tree\n",
        "start = time.time()\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "tree_time = time.time() - start\n",
        "\n",
        "# Time a Random Forest (ensemble)\n",
        "start = time.time()\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "forest_time = time.time() - start\n",
        "\n",
        "print(f\"Decision Tree training time: {tree_time:.4f} seconds\")\n",
        "print(f\"Random Forest training time: {forest_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoS-tgXzD6mQ",
        "outputId": "0cf26768-9981-4acc-c056-8c8544ad5771"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree training time: 0.0027 seconds\n",
            "Random Forest training time: 0.0761 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "inCWnnuEOm0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [10, 50, 100],  # Number of trees\n",
        "    \"max_depth\": [None, 5, 10],  # Maximum depth of trees\n",
        "    \"min_samples_split\": [2, 5, 10]  # Minimum number of samples required to split\n",
        "}\n",
        "\n",
        "# Initialize Random Forest and GridSearchCV\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wughY_8ljML",
        "outputId": "59bd40f4-c63a-468a-a172-9e34ed15733f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 10}\n",
            "Best Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n"
      ],
      "metadata": {
        "id": "W8P4MVdnljpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier using Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Updated for latest scikit-learn\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9TVfFCrlm3i",
        "outputId": "e24ed6fd-6158-4a8d-8ea0-b1548918b101"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "4olE4Kdalnih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Retrieve feature importances\n",
        "importances = rf.feature_importances_\n",
        "features = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Display feature importances\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0boe9tzc2Ugu",
        "outputId": "497481d1-c40b-4a00-f661-c549f77add36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Feature  Importance\n",
            "23  Feature 23    0.139357\n",
            "27  Feature 27    0.132225\n",
            "7    Feature 7    0.107046\n",
            "20  Feature 20    0.082848\n",
            "22  Feature 22    0.080850\n",
            "2    Feature 2    0.067990\n",
            "6    Feature 6    0.066917\n",
            "3    Feature 3    0.060462\n",
            "26  Feature 26    0.037339\n",
            "0    Feature 0    0.034843\n",
            "13  Feature 13    0.029553\n",
            "25  Feature 25    0.019864\n",
            "21  Feature 21    0.017485\n",
            "1    Feature 1    0.015225\n",
            "10  Feature 10    0.014264\n",
            "24  Feature 24    0.012232\n",
            "5    Feature 5    0.011597\n",
            "12  Feature 12    0.010085\n",
            "28  Feature 28    0.008179\n",
            "4    Feature 4    0.007958\n",
            "19  Feature 19    0.005942\n",
            "16  Feature 16    0.005820\n",
            "15  Feature 15    0.005612\n",
            "14  Feature 14    0.004722\n",
            "29  Feature 29    0.004497\n",
            "17  Feature 17    0.003760\n",
            "11  Feature 11    0.003744\n",
            "18  Feature 18    0.003546\n",
            "8    Feature 8    0.003423\n",
            "9    Feature 9    0.002615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "hCOHkoy-2X-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor using Decision Trees\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute Mean Squared Error (MSE)\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAl2lvd2by8",
        "outputId": "70be7073-6659-4bef-ec76-2aa0a77a4a9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 871.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "qaHiYZxW2cOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute Confusion Matrix\n",
        "y_pred = rf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix of Random Forest Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ZTZCevz12hRN",
        "outputId": "9fd1db42-0082-4c01-abca-6857993e706b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "hrIV6tuM2hs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "]\n",
        "\n",
        "# Define stacking ensemble with Logistic Regression as final estimator\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfEeCVRC2lmJ",
        "outputId": "3d0e6bef-81fd-4764-f9a8-3b50c6e54c36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "VwByvQVH2nwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get top 5 important features\n",
        "importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Features:\")\n",
        "print(importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhIFxnnP2rOi",
        "outputId": "3d24a76d-3583-4d27-a6a2-3ba5e64852ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "OGTxNhdI2rkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T96o6Q-m2wZH",
        "outputId": "d9b1bc33-3f72-416f-c490-e2b5ffe6c9e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "EotJHKwJ2w2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare models with different max depths\n",
        "for depth in [None, 5, 10]:\n",
        "    rf = RandomForestClassifier(n_estimators=50, max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    print(f\"Random Forest (Max Depth {depth}) Accuracy: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m_61Cdt21Hv",
        "outputId": "38a5059b-c97d-4e45-86a3-7ba42de57389"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (Max Depth None) Accuracy: 1.00\n",
            "Random Forest (Max Depth 5) Accuracy: 1.00\n",
            "Random Forest (Max Depth 10) Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance."
      ],
      "metadata": {
        "id": "hrv9LIpg22Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different base estimators\n",
        "for estimator in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    bagging_reg = BaggingRegressor(estimator=estimator, n_estimators=10, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {estimator.__class__.__name__} MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdDANTTq29RL",
        "outputId": "b4ba02bb-eac5-44e5-e7b9-1ce79ca18957"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with DecisionTreeRegressor MSE: 871.72\n",
            "Bagging Regressor with KNeighborsRegressor MSE: 1171.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "IxgnS8aF29oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and compute AUC score\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Random Forest ROC-AUC Score: {auc_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfdXvHkP3Di7",
        "outputId": "54a1256f-dec9-4cc7-d44b-71d0a6af59a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "WM8kc8Dn3ECR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Compute cross-validation scores\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y87M3soG3G4F",
        "outputId": "8a47fedb-9069-41fc-adfc-060614b63f0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
            "Mean Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curv."
      ],
      "metadata": {
        "id": "QTzUr46o3Hkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Random Forest')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KVeS6dGg3LCH",
        "outputId": "d65fc291-4c7d-432f-98be-d9e4446f1f9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "w_YHIV8w3LdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define stacking ensemble\n",
        "stacking_clf = StackingClassifier(estimators=[\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42))\n",
        "], final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frZMaAf-3OQQ",
        "outputId": "3aebe57f-2ce6-4a68-b1b8-883a5510e358"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "8-Uw2KhA3Opt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compare different bootstrap sample sizes\n",
        "for max_samples in [0.5, 0.8, 1.0]:\n",
        "    bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, max_samples=max_samples, random_state=42)\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor (max_samples={max_samples}) MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n_Fw8pC3QSp",
        "outputId": "32ced56a-75ca-4ced-c9fc-5103b3aba564"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor (max_samples=0.5) MSE: 1154.52\n",
            "Bagging Regressor (max_samples=0.8) MSE: 936.86\n",
            "Bagging Regressor (max_samples=1.0) MSE: 871.72\n"
          ]
        }
      ]
    }
  ]
}